{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Donald Trump Speeches: https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt\n",
    "with open(\"speeches.txt\", 'r', encoding='utf8', errors='ignore') as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "speech_text = \"\"\n",
    "for line in text:\n",
    "    if line.strip() and not (line.strip().startswith(\"SPEECH \")):\n",
    "        speech_text += \" \" + line.strip()\n",
    "\n",
    "sentences = nltk.sent_tokenize(speech_text)\n",
    "corpus = []\n",
    "for sent in sentences:\n",
    "    sent = sent.replace(\"...\", \" \")\n",
    "    sent = sent.replace(\"--\", \" \")\n",
    "    sent = sent.replace(\"’\", \"'\")\n",
    "    sent = sent.replace(\"—\", \"-\")\n",
    "    sent = sent.replace(\"``\", \"'\")\n",
    "    if sent.isupper():\n",
    "        sent = sent[0] + sent[1:].lower()\n",
    "    corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = defaultdict()\n",
    "processed_corpus = []\n",
    "\n",
    "def preprocess_and_construct_vocab(corpus):\n",
    "    for sent in corpus:\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        lowercased_tokens = [x.lower() for x in tokens]\n",
    "        sent_tokens = [\"<s>\"] + lowercased_tokens + [\"</s>\"]\n",
    "        for tok in sent_tokens:\n",
    "            if tok in vocab:\n",
    "                vocab[tok] += 1\n",
    "            else:\n",
    "                vocab[tok] = 1\n",
    "        processed_corpus.append(sent_tokens)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  13193\n",
      "Test set size: 3299\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_construct_vocab(corpus)\n",
    "\n",
    "train, test = train_test_split(processed_corpus, test_size=0.2)\n",
    "\n",
    "print(\"Train set size: \", len(train))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams(sent_tokens, n):\n",
    "    return ngrams(sent_tokens, n)\n",
    "\n",
    "def compute_all_possible_ngram_count(vocab_size, n):\n",
    "    return vocab_size**n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "quadgrams = Counter()\n",
    "\n",
    "for sent_toks in train:\n",
    "    bigrams.update([bg for bg in compute_ngrams(sent_toks, 2)])\n",
    "    trigrams.update([tg for tg in compute_ngrams(sent_toks, 3)])\n",
    "    quadgrams.update([qg for qg in compute_ngrams(sent_toks, 4)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  6152\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also the count of unique unigrams in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall bigram count:  177055\n",
      "Count of unique bigrams:  41665\n",
      "All possible bigrams in the corpus:  37847104\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall bigram count: \", sum(bigrams.values()))\n",
    "print(\"Count of unique bigrams: \", len(bigrams))\n",
    "print(\"All possible bigrams in the corpus: \", compute_all_possible_ngram_count(len(vocab), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall trigram count:  163862\n",
      "Count of unique trigrams:  87090\n",
      "All possible trigrams in the corpus:  232835383808\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall trigram count: \", sum(trigrams.values()))\n",
    "print(\"Count of unique trigrams: \", len(trigrams))\n",
    "print(\"All possible trigrams in the corpus: \", compute_all_possible_ngram_count(len(vocab), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadgram Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall quadgram count:  150672\n",
      "Count of unique quadgrams:  112493\n",
      "All possible trigrams in the corpus:  1432403281186816\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall quadgram count: \", sum(quadgrams.values()))\n",
    "print(\"Count of unique quadgrams: \", len(quadgrams))\n",
    "print(\"All possible trigrams in the corpus: \", compute_all_possible_ngram_count(len(vocab), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unigram model, $P(w_n | w_{1}^{n-1}) = P(w_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Maximum Likelihood Estimation, $P(w_i) = \\frac{\\text{count}(w_i)}{\\sum_{w} \\text{count}(w)} = \\frac{\\text{count}(w_i)}{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Add One Smoothing, $P(w_i) = \\frac{\\text{count}(w_i) + 1}{N + V}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    # Using MLE with Add One Smoothing\n",
    "    def conditional_prob_of_tokens(self, unigr):\n",
    "        unigr = unigr[0]\n",
    "        return ((vocab[unigr] + 1)/(sum(vocab.values()) + len(vocab)))\n",
    "    \n",
    "    def perplexity_sent(self, sent_toks):\n",
    "        perplexity = 1.0\n",
    "        \n",
    "        sent_unigrams = compute_ngrams(sent_toks, 1)\n",
    "        for ug in sent_unigrams:\n",
    "            perplexity = perplexity * (1/self.conditional_prob_of_tokens(ug))\n",
    "            \n",
    "        perplexity = perplexity ** (1/len(sent_toks))\n",
    "        if perplexity == math.inf:\n",
    "            #print(sent_toks)\n",
    "            perplexity = 0.0\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Maximum Likelihood Estimation, $P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\sum_{w} \\text{count}(w_{i-1}, w)} = \\frac{\\text{count}(w_{i-1}, w_i)}{ \\text{count}(w_{i-1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Add One Smoothing, $P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i) + 1}{ \\text{count}(w_{i-1}) + V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel:\n",
    "    \n",
    "    def MLE_with_add_one_smoothing(self, w_i_minus_1, w_i):\n",
    "        co_occurrence_count = bigrams[(w_i_minus_1, w_i)] + 1\n",
    "        unigram_count = vocab[w_i_minus_1] + len(vocab)\n",
    "        return co_occurrence_count/unigram_count\n",
    "    \n",
    "    def conditional_prob_of_tokens(self, bigram_tuple):\n",
    "        return self.MLE_with_add_one_smoothing(bigram_tuple[0], bigram_tuple[1])\n",
    "    \n",
    "    def perplexity_sent(self, sent_toks):\n",
    "        perplexity = 1.0\n",
    "        \n",
    "        sent_bigrams = compute_ngrams(sent_toks, 2)\n",
    "        for bg in sent_bigrams:\n",
    "            perplexity = perplexity * (1/self.conditional_prob_of_tokens(bg))\n",
    "            \n",
    "        perplexity = perplexity ** (1/len(sent_toks))\n",
    "        if perplexity == math.inf:\n",
    "            #print(sent_toks)\n",
    "            perplexity = 0.0\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Maximum Likelihood Estimation and Add One Smoothing, $P(w_i | w_{i-2}, w_{i-1}) = \\frac{\\text{count}(w_{i-2}, w_{i-1}, w_i) + 1}{ \\text{count}(w_{i-2}, w_{i-1}) + V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel:\n",
    "    \n",
    "    def MLE_with_add_one_smoothing(self, w_i_minus_2, w_i_minus_1, w_i):\n",
    "        co_occurrence_count = trigrams[(w_i_minus_2, w_i_minus_1, w_i)] + 1\n",
    "        bigram_count = bigrams[(w_i_minus_2, w_i_minus_1)] + len(vocab)\n",
    "        return co_occurrence_count/bigram_count\n",
    "    \n",
    "    def conditional_prob_of_tokens(self, trigram_tuple):\n",
    "        return self.MLE_with_add_one_smoothing(trigram_tuple[0], trigram_tuple[1], trigram_tuple[2])\n",
    "    \n",
    "    def perplexity_sent(self, sent_toks):\n",
    "        perplexity = 1.0\n",
    "        \n",
    "        sent_bigrams = compute_ngrams(sent_toks, 3)\n",
    "        for bg in sent_bigrams:\n",
    "            perplexity = perplexity * (1/self.conditional_prob_of_tokens(bg))\n",
    "            \n",
    "        perplexity = perplexity ** (1/len(sent_toks))\n",
    "        if perplexity == math.inf:\n",
    "            #print(sent_toks)\n",
    "            perplexity = 0.0\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadgram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Maximum Likelihood Estimation and Add One Smoothing, $P(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{\\text{count}(w_{i-3}, w_{i-2}, w_{i-1}, w_i) + 1}{ \\text{count}(w_{i-3}, w_{i-2}, w_{i-1}) + V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadgramModel:\n",
    "    \n",
    "    def MLE_with_add_one_smoothing(self, w_i_minus_3, w_i_minus_2, w_i_minus_1, w_i):\n",
    "        co_occurrence_count = quadgrams[(w_i_minus_3, w_i_minus_2, w_i_minus_1, w_i)] + 1\n",
    "        trigram_count = trigrams[(w_i_minus_3, w_i_minus_2, w_i_minus_1)] + len(vocab)\n",
    "        return co_occurrence_count/trigram_count\n",
    "    \n",
    "    def conditional_prob_of_tokens(self, quadgram_tuple):\n",
    "        return self.MLE_with_add_one_smoothing(quadgram_tuple[0], quadgram_tuple[1], quadgram_tuple[2], quadgram_tuple[2])\n",
    "    \n",
    "    def perplexity_sent(self, sent_toks):\n",
    "        perplexity = 1.0\n",
    "        \n",
    "        sent_bigrams = compute_ngrams(sent_toks, 4)\n",
    "        for bg in sent_bigrams:\n",
    "            perplexity = perplexity * (1/self.conditional_prob_of_tokens(bg))\n",
    "            \n",
    "        perplexity = perplexity ** (1/len(sent_toks))\n",
    "        if perplexity == math.inf:\n",
    "            #print(sent_toks)\n",
    "            perplexity = 0.0\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: Perplexity on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity of the Unirgam model on the test dataset:  238.21660075056752\n",
      "Average perplexity of the Birgam model on the test dataset:  226.22457207989984\n",
      "Average perplexity of the Unirgam model on the test dataset:  590.0011143806028\n",
      "Average perplexity of the Birgam model on the test dataset:  939.1964927310736\n"
     ]
    }
   ],
   "source": [
    "unigram_model = UnigramModel()\n",
    "bigram_model = BigramModel()\n",
    "trigram_model = TrigramModel()\n",
    "quadgram_model = QuadgramModel()\n",
    "\n",
    "uni_test_pps = []\n",
    "bi_test_pps = []\n",
    "tri_test_pps = []\n",
    "quad_test_pps = []\n",
    "\n",
    "for sent_toks in test:\n",
    "    uni_test_pps.append(unigram_model.perplexity_sent(sent_toks))\n",
    "    bi_test_pps.append(bigram_model.perplexity_sent(sent_toks))\n",
    "    tri_test_pps.append(trigram_model.perplexity_sent(sent_toks))\n",
    "    quad_test_pps.append(quadgram_model.perplexity_sent(sent_toks))\n",
    "    \n",
    "print(\"Average perplexity of the Unirgam model on the test dataset: \", np.mean(uni_test_pps))\n",
    "print(\"Average perplexity of the Birgam model on the test dataset: \", np.mean(bi_test_pps))\n",
    "print(\"Average perplexity of the Unirgam model on the test dataset: \", np.mean(tri_test_pps))\n",
    "print(\"Average perplexity of the Birgam model on the test dataset: \", np.mean(quad_test_pps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram model is the naive model that calculates perplexity based on the frequency counts of individual words only, and bigram is a better model than the unigram model. The perplexity values for the trigram and the quadgram model are increasing as we are increasing the value of n, as our model gets more and more confused when it sees a longer stream of tokens that it hasn't seen before. We can improve these results by using advanced smoothing methods such as Good Turing with Interpolation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
